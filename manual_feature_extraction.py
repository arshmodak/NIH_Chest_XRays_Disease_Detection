# -*- coding: utf-8 -*-
"""manual_feature_extraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UctZyUnhnCxzAZy8UY5G6Nds4YbQDZ-l
"""

!pip install PyWavelets

from google.colab import drive
drive.mount('/content/drive')

!cp "/content/drive/MyDrive/CS5100 Project/resized_images_v4_224.zip" -r "/content/resized_images_v4_224.zip"
!unzip /content/resized_images_v4_224.zip -d /content/resized_images_v4_224

import pickle
def read_pickle_file(filepath):
  with open(filepath, "rb") as f:
    return pickle.load(f)

filepath = "/content/class_dict_wo_NF.pkl"
class_dict = read_pickle_file(filepath)

df = pd.read_csv("/content/final_df_all_images_wo_NF_labels.csv")

df.drop(columns=["disease_vec"], axis=1, inplace=True)
df['disease_vec'] = df.apply(lambda x: [x[class_dict.values()].values], 1).map(lambda x: x[0])
df["disease_vec"] = df["disease_vec"].apply(lambda x: np.array(x, dtype=int))

all_labels = list(class_dict.values())
# all_labels
total_data = 0
for c_label in all_labels:
  print("{}: {}".format(c_label, df[c_label].sum()))
  total_data += df[c_label].sum()

print("\n")
print("Total Size of Data: {}".format(total_data))

df.head()

df["image_path"] = "/content/resized_images_v4_224/resized_images_v4_224/" + df["Image Index"]

X, y = df["image_path"], df["disease_vec"]

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101, shuffle=True)
X_train, X_test = pd.DataFrame(X_train), pd.DataFrame(X_test)
train_df = pd.concat([X_train, y_train], axis=1)
test_df = pd.concat([X_test, y_test], axis=1).reset_index().drop("index", axis=1)

rs = np.random.RandomState(101)
msk = rs.rand(len(train_df)) < 0.85
valid_df = train_df[~msk].reset_index().drop("index", axis=1)
train_df = train_df[msk].reset_index().drop("index", axis=1)

# HOG - Histogram of Oriented Gradients
from skimage.io import imread
from skimage.transform import resize
from skimage.feature import hog
from skimage import exposure
import matplotlib.pyplot as plt
from skimage.color import rgb2gray

def hog_features(path):
  # reading the image
  img = imread(path)
  img = rgb2gray(img)
  resized_img = resize(img, (224,224))
  exposure_balanced_image = exposure.rescale_intensity(resized_img, in_range=(0, 225), out_range=(0, 225))
  fd = hog(exposure_balanced_image, orientations=4, pixels_per_cell=(8, 8),
                	cells_per_block=(1, 1), visualize=False, multichannel=False)
  return (fd.tolist() , exposure_balanced_image)

# LBP- Local Binary Pattern
from skimage.feature import local_binary_pattern
from skimage import data

def LBP(exposure_balanced_image, radius = 3):

  def hist(lbp):
    n_bins = int(lbp.max() + 1)
    return plt.hist(lbp.ravel(), density=True, bins=n_bins, range=(0, n_bins),facecolor='0.5')

  n_points = 8 * radius
  lbp = local_binary_pattern(exposure_balanced_image, n_points, radius, method = 'uniform')
  histogram = hist(lbp)
  plt.close()
  histogram = histogram[0].tolist()
  return histogram

# Gray level co-occurance matrix

import numpy as np
from skimage.feature import greycomatrix, greycoprops
from skimage import io, color, img_as_ubyte

def GLCM(image):
  image = img_as_ubyte(image)

  bins = np.array([0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 255]) #16-bit
  inds = np.digitize(image, bins)

  max_value = inds.max()+1
  matrix_coocurrence = greycomatrix(inds, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4], levels=max_value, normed=False, symmetric=False)

  # GLCM properties
  def contrast_feature(matrix_coocurrence):
      contrast = greycoprops(matrix_coocurrence, 'contrast')
      return contrast

  def dissimilarity_feature(matrix_coocurrence):
      dissimilarity = greycoprops(matrix_coocurrence, 'dissimilarity')    
      return dissimilarity

  def homogeneity_feature(matrix_coocurrence):
      homogeneity = greycoprops(matrix_coocurrence, 'homogeneity')
      return homogeneity

  def energy_feature(matrix_coocurrence):
      energy = greycoprops(matrix_coocurrence, 'energy')
      return energy

  def correlation_feature(matrix_coocurrence):
      correlation = greycoprops(matrix_coocurrence, 'correlation')
      return correlation

  def entropy_feature(matrix_coocurrence):
      entropy = greycoprops(matrix_coocurrence, 'entropy')
      return entropy

  # print(contrast_feature(matrix_coocurrence))
  # print(dissimilarity_feature(matrix_coocurrence))
  # print(homogeneity_feature(matrix_coocurrence))
  # print(energy_feature(matrix_coocurrence))
  # print(correlation_feature(matrix_coocurrence))
  return np.concatenate((contrast_feature(matrix_coocurrence),dissimilarity_feature(matrix_coocurrence),homogeneity_feature(matrix_coocurrence),energy_feature(matrix_coocurrence),correlation_feature(matrix_coocurrence)), axis = 1)

import pywt
import pywt.data
def wavelet_transform(image):

  coef = pywt.wavedec2(image, 'db1' , level=4) # level3 decomposition #use db1 or haar

  cA = coef[0]
  (cH1,cV1,cD1) =  coef[-1]#level1
  (cH2,cV2,cD2) =  coef[-2]#level2
  (cH3,cV3,cD3) =  coef[-3]#level3
  (cH4,cV4,cD4) =  coef[-4]#level4 output of level4 decomp and furthur level dont offer better features
  return cA.ravel().tolist()+cH4.ravel().tolist()+cV4.ravel().tolist()+cD4.ravel().tolist()

path = '/content/00000075_000.png'
fd, exposure_balanced_image = hog_features(path)
hist = LBP(exposure_balanced_image)
covariance_matrix = GLCM(exposure_balanced_image).tolist()[0]
wt = wavelet_transform(exposure_balanced_image)
lst = fd + covariance_matrix + hist + wt
df = pd.DataFrame(lst)

def extract_features(dataset):
  return_df = pd.DataFrame()
  for i in range(dataset.shape[0]):
    lst = list()
    path = dataset['image_path'][i]
    fd, exposure_balanced_image = hog_features(path)
    hist = LBP(exposure_balanced_image)
    covariance_matrix = GLCM(exposure_balanced_image).tolist()[0]
    wt = wavelet_transform(exposure_balanced_image)
    lst = fd + covariance_matrix + hist 
    df = pd.DataFrame(lst)
    return_df = return_df.append(df)
  
  return_df['disease_vec'] = dataset['disease_vec']
  return return_df

train_df = extract_features(train_df)
test_df = extract_features(test_df)
valid_df = extract_features(valid_df)

with open("/content/drive/MyDrive/CS5100 Project/manual_features.pkl", "wb") as fp:
  pickle.dump([train_df,test_df,val_df], fp)

train_df.head()

